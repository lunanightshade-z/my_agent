# ArXiv Fetcher - ArXivè®ºæ–‡æ£€ç´¢å·¥å…·

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªç®€å•æ˜“ç”¨çš„ArXivè®ºæ–‡æ£€ç´¢å·¥å…·ï¼Œæ”¯æŒåŸºäºå…³é”®è¯æ£€ç´¢ä»»æ„æ•°é‡çš„è®ºæ–‡ï¼Œå¹¶è·å–è®ºæ–‡çš„æ ‡é¢˜ã€æ‘˜è¦ã€ä½œè€…ã€åˆ†ç±»ç­‰è¯¦ç»†ä¿¡æ¯ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

- âœ… **å…³é”®è¯æ£€ç´¢**: æ”¯æŒåœ¨æ‰€æœ‰å­—æ®µæˆ–ä»…åœ¨æ ‡é¢˜ä¸­æœç´¢
- âœ… **çµæ´»æ•°é‡**: æ”¯æŒæ£€ç´¢ä»»æ„æ•°é‡çš„è®ºæ–‡ï¼ˆ1-30000ç¯‡ï¼‰
- âœ… **å®Œæ•´ä¿¡æ¯**: è·å–è®ºæ–‡æ ‡é¢˜ã€æ‘˜è¦ã€ä½œè€…ã€å‘å¸ƒæ—¥æœŸã€åˆ†ç±»ã€PDFé“¾æ¥ç­‰
- âœ… **å¤šç§æ’åº**: æ”¯æŒæŒ‰ç›¸å…³æ€§ã€æ›´æ–°æ—¶é—´ã€æäº¤æ—¶é—´æ’åº
- âœ… **JSONå¯¼å‡º**: æ”¯æŒå°†ç»“æœå¯¼å‡ºä¸ºJSONæ ¼å¼
- âœ… **é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œå‚æ•°éªŒè¯
- âœ… **æ—¥å¿—è®°å½•**: è¯¦ç»†çš„æ—¥å¿—è¾“å‡ºï¼Œä¾¿äºè°ƒè¯•

## ğŸ“¦ ä¾èµ–

é¡¹ç›®å·²åŒ…å«ä»¥ä¸‹ä¾èµ–ï¼š
- `requests`: HTTPè¯·æ±‚åº“

æ— éœ€é¢å¤–å®‰è£…ä¾èµ–ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```python
from backend.tools.arxiv_fetcher import ArxivFetcher

# åˆ›å»ºæ£€ç´¢å™¨
fetcher = ArxivFetcher()

# æœç´¢è®ºæ–‡ï¼ˆåœ¨æ‰€æœ‰å­—æ®µä¸­æœç´¢ï¼‰
result = fetcher.search_by_keywords("machine learning", max_results=10)

# éå†ç»“æœ
for paper in result.papers:
    print(f"æ ‡é¢˜: {paper.title}")
    print(f"ä½œè€…: {', '.join(paper.authors)}")
    print(f"æ‘˜è¦: {paper.summary[:200]}...")
    print(f"é“¾æ¥: {paper.arxiv_url}")
    print("-" * 80)
```

### é«˜çº§æŸ¥è¯¢

```python
# åªåœ¨æ ‡é¢˜ä¸­æœç´¢
result = fetcher.search("ti:transformer", max_results=5)

# ä½¿ç”¨ArXivæŸ¥è¯¢è¯­æ³•
result = fetcher.search("all:deep learning AND cat:cs.LG", max_results=20)

# æŒ‰æ›´æ–°æ—¶é—´æ’åº
result = fetcher.search(
    "all:neural network",
    max_results=10,
    sort_by="lastUpdatedDate",
    sort_order="descending"
)
```

### å¯¼å‡ºJSON

```python
import json

result = fetcher.search_by_keywords("reinforcement learning", max_results=5)

# è½¬æ¢ä¸ºå­—å…¸
result_dict = result.to_dict()

# ä¿å­˜åˆ°æ–‡ä»¶
with open("papers.json", "w", encoding="utf-8") as f:
    json.dump(result_dict, f, indent=2, ensure_ascii=False)
```

### ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨

```python
# è‡ªåŠ¨ç®¡ç†èµ„æº
with ArxivFetcher() as fetcher:
    result = fetcher.search_by_keywords("computer vision", max_results=10)
    # å¤„ç†ç»“æœ...
```

## ğŸ“– APIæ–‡æ¡£

### ArxivFetcher

#### `search(query, max_results=10, sort_by="relevance", sort_order="descending")`

æœç´¢ArXivè®ºæ–‡ã€‚

**å‚æ•°:**
- `query` (str): æœç´¢æŸ¥è¯¢ï¼ˆæ”¯æŒArXivæŸ¥è¯¢è¯­æ³•ï¼‰
- `max_results` (int): æœ€å¤§è¿”å›ç»“æœæ•°ï¼ˆ1-30000ï¼Œé»˜è®¤10ï¼‰
- `sort_by` (str): æ’åºæ–¹å¼ï¼Œå¯é€‰å€¼ï¼š
  - `"relevance"`: æŒ‰ç›¸å…³æ€§æ’åºï¼ˆé»˜è®¤ï¼‰
  - `"lastUpdatedDate"`: æŒ‰æ›´æ–°æ—¶é—´æ’åº
  - `"submittedDate"`: æŒ‰æäº¤æ—¶é—´æ’åº
- `sort_order` (str): æ’åºé¡ºåºï¼Œ`"ascending"` æˆ– `"descending"`ï¼ˆé»˜è®¤ï¼‰

**è¿”å›:** `ArxivSearchResult` å¯¹è±¡

**ç¤ºä¾‹:**
```python
result = fetcher.search("all:machine learning", max_results=20)
```

#### `search_by_keywords(keywords, max_results=10, search_all_fields=True)`

é€šè¿‡å…³é”®è¯æœç´¢è®ºæ–‡ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰ã€‚

**å‚æ•°:**
- `keywords` (str): æœç´¢å…³é”®è¯
- `max_results` (int): æœ€å¤§è¿”å›ç»“æœæ•°
- `search_all_fields` (bool): æ˜¯å¦åœ¨æ‰€æœ‰å­—æ®µä¸­æœç´¢
  - `True`: åœ¨æ‰€æœ‰å­—æ®µä¸­æœç´¢ï¼ˆ`all:keywords`ï¼‰
  - `False`: ä»…åœ¨æ ‡é¢˜ä¸­æœç´¢ï¼ˆ`ti:keywords`ï¼‰

**è¿”å›:** `ArxivSearchResult` å¯¹è±¡

**ç¤ºä¾‹:**
```python
# åœ¨æ‰€æœ‰å­—æ®µä¸­æœç´¢
result = fetcher.search_by_keywords("transformer", max_results=10)

# ä»…åœ¨æ ‡é¢˜ä¸­æœç´¢
result = fetcher.search_by_keywords("transformer", max_results=10, search_all_fields=False)
```

### ArxivPaper

è®ºæ–‡æ•°æ®æ¨¡å‹ï¼ŒåŒ…å«ä»¥ä¸‹å±æ€§ï¼š

- `arxiv_id` (str): ArXiv ID
- `title` (str): è®ºæ–‡æ ‡é¢˜
- `summary` (str): è®ºæ–‡æ‘˜è¦
- `authors` (List[str]): ä½œè€…åˆ—è¡¨
- `published` (str): å‘å¸ƒæ—¥æœŸï¼ˆISOæ ¼å¼ï¼‰
- `updated` (str): æ›´æ–°æ—¥æœŸï¼ˆISOæ ¼å¼ï¼‰
- `pdf_url` (str): PDFä¸‹è½½é“¾æ¥
- `arxiv_url` (str): ArXivé¡µé¢é“¾æ¥
- `primary_category` (str): ä¸»è¦åˆ†ç±»
- `categories` (List[str]): æ‰€æœ‰åˆ†ç±»åˆ—è¡¨
- `comment` (str): è¯„è®º/å¤‡æ³¨

**æ–¹æ³•:**
- `to_dict()`: è½¬æ¢ä¸ºå­—å…¸æ ¼å¼

### ArxivSearchResult

æ£€ç´¢ç»“æœæ¨¡å‹ï¼ŒåŒ…å«ä»¥ä¸‹å±æ€§ï¼š

- `query` (str): æŸ¥è¯¢å…³é”®è¯
- `total_results` (int): æ€»ç»“æœæ•°
- `papers` (List[ArxivPaper]): è®ºæ–‡åˆ—è¡¨
- `search_time` (str): æ£€ç´¢æ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰

**æ–¹æ³•:**
- `to_dict()`: è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
- `get_papers()`: è·å–æ‰€æœ‰è®ºæ–‡åˆ—è¡¨

## ğŸ” ArXivæŸ¥è¯¢è¯­æ³•

ArXivæ”¯æŒä¸°å¯Œçš„æŸ¥è¯¢è¯­æ³•ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å¸¸ç”¨ç¤ºä¾‹ï¼š

- `all:machine learning`: åœ¨æ‰€æœ‰å­—æ®µä¸­æœç´¢"machine learning"
- `ti:transformer`: åœ¨æ ‡é¢˜ä¸­æœç´¢"transformer"
- `au:Smith`: æœç´¢ä½œè€…ååŒ…å«"Smith"çš„è®ºæ–‡
- `cat:cs.LG`: æœç´¢è®¡ç®—æœºç§‘å­¦-æœºå™¨å­¦ä¹ åˆ†ç±»çš„è®ºæ–‡
- `all:deep learning AND cat:cs.CV`: æœç´¢åŒ…å«"deep learning"ä¸”å±äºè®¡ç®—æœºè§†è§‰åˆ†ç±»çš„è®ºæ–‡
- `all:neural network OR all:deep learning`: æœç´¢åŒ…å«"neural network"æˆ–"deep learning"çš„è®ºæ–‡

æ›´å¤šæŸ¥è¯¢è¯­æ³•è¯·å‚è€ƒ [ArXiv APIæ–‡æ¡£](https://arxiv.org/help/api/user-manual#query_details)ã€‚

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: æ£€ç´¢æœ€æ–°è®ºæ–‡

```python
from backend.tools.arxiv_fetcher import ArxivFetcher

fetcher = ArxivFetcher()

# æ£€ç´¢æœ€æ–°çš„10ç¯‡æœºå™¨å­¦ä¹ è®ºæ–‡
result = fetcher.search(
    "all:machine learning",
    max_results=10,
    sort_by="submittedDate",
    sort_order="descending"
)

for paper in result.papers:
    print(f"{paper.published}: {paper.title}")
```

### ç¤ºä¾‹2: æ£€ç´¢ç‰¹å®šä½œè€…çš„è®ºæ–‡

```python
result = fetcher.search("au:Yann LeCun", max_results=5)

for paper in result.papers:
    print(f"{paper.title}")
    print(f"ä½œè€…: {', '.join(paper.authors)}")
```

### ç¤ºä¾‹3: æ‰¹é‡æ£€ç´¢å¹¶ä¿å­˜

```python
import json
from pathlib import Path

keywords = ["transformer", "BERT", "GPT"]
all_papers = []

fetcher = ArxivFetcher()

for keyword in keywords:
    result = fetcher.search_by_keywords(keyword, max_results=20)
    all_papers.extend(result.papers)
    print(f"æ£€ç´¢ '{keyword}': {len(result.papers)} ç¯‡è®ºæ–‡")

# ä¿å­˜æ‰€æœ‰è®ºæ–‡
output_file = Path("all_papers.json")
with open(output_file, "w", encoding="utf-8") as f:
    papers_dict = [paper.to_dict() for paper in all_papers]
    json.dump(papers_dict, f, indent=2, ensure_ascii=False)

print(f"\nå…±æ£€ç´¢åˆ° {len(all_papers)} ç¯‡è®ºæ–‡ï¼Œå·²ä¿å­˜åˆ° {output_file}")
```

## ğŸ§ª æµ‹è¯•

è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š

```bash
cd backend
python3 tests/test_arxiv_fetcher.py
```

æµ‹è¯•è„šæœ¬åŒ…å«ä»¥ä¸‹æµ‹è¯•ï¼š
1. åŸºæœ¬å…³é”®è¯æœç´¢
2. è‡ªå®šä¹‰æŸ¥è¯¢è¯­æ³•
3. æ£€ç´¢å¤§é‡è®ºæ–‡
4. JSONå¯¼å‡º
5. é”™è¯¯å¤„ç†

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **APIé™åˆ¶**: ArXiv APIå¯¹è¯·æ±‚é¢‘ç‡æœ‰é™åˆ¶ï¼Œå»ºè®®åœ¨è¯·æ±‚ä¹‹é—´æ·»åŠ é€‚å½“å»¶è¿Ÿ
2. **ç½‘ç»œè¿æ¥**: éœ€è¦èƒ½å¤Ÿè®¿é—® `http://export.arxiv.org`
3. **ç»“æœæ•°é‡**: å•æ¬¡æŸ¥è¯¢æœ€å¤šè¿”å›30000æ¡ç»“æœ
4. **è¶…æ—¶è®¾ç½®**: é»˜è®¤è¶…æ—¶æ—¶é—´ä¸º30ç§’ï¼Œå¯ä»¥é€šè¿‡æ„é€ å‡½æ•°å‚æ•°è°ƒæ•´

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªé¡¹ç›®ä¸»è®¸å¯è¯ã€‚
