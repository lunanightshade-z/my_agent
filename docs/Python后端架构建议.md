
## 核心原则
**可观测性（Observability）** 是首要原则。
生产系统必须能够在问题发生时快速定位，而不是被动地等待报错。这包括结构化日志、分布式追踪、指标收集，三个层面的完整覆盖。
**可靠性与容错** 是第二位的。
系统需要优雅降级、重试机制、超时控制、断路器模式。特别是对于你现在做的AI Agent系统，外部依赖（LLM API、数据库等）的失败是常态，需要有完善的处理策略。
**可维护性** 体现在清晰的分层、接口契约的明确定义、充分的测试覆盖。
**性能和成本** 包括资源利用率、并发能力、缓存策略。对于AI应用来说，通常成本由API调用次数主导，所以缓存和请求合并变得特别重要。

## 推荐架构设计
### 1. 分层架构

```
┌─────────────────────────────────────┐
│         API Layer (FastAPI)         │
│    - Request validation             │
│    - Response serialization         │
│    - Authentication/Authorization   │
└──────────────┬──────────────────────┘
               │
┌──────────────▼──────────────────────┐
│       Business Logic Layer          │
│    - Domain models                  │
│    - Use cases / Services           │
│    - Orchestration logic            │
└──────────────┬──────────────────────┘
               │
┌──────────────▼──────────────────────┐
│     Data Access & Integration       │
│    - Repository pattern             │
│    - External API clients           │
│    - Database queries               │
└──────────────┬──────────────────────┘
               │
┌──────────────▼──────────────────────┐
│      Infrastructure Layer           │
│    - Logging, tracing               │
│    - Caching, queuing               │
│    - Configuration management       │
└─────────────────────────────────────┘
```

### 2. 完整的代码结构示例

```
project/
├── app/
│   ├── __init__.py
│   ├── main.py                 # FastAPI应用入口
│   ├── config.py               # 配置管理
│   ├── dependencies.py         # 依赖注入
│   │
│   ├── api/                    # API层
│   │   ├── __init__.py
│   │   ├── v1/
│   │   │   ├── __init__.py
│   │   │   ├── agents.py       # agent相关的endpoints
│   │   │   └── tasks.py        # task相关的endpoints
│   │   └── schemas.py          # Pydantic models
│   │
│   ├── core/                   # 核心业务逻辑
│   │   ├── __init__.py
│   │   ├── agent/
│   │   │   ├── __init__.py
│   │   │   ├── base.py         # agent基础类
│   │   │   ├── orchestrator.py # agent协调
│   │   │   └── executor.py     # agent执行
│   │   └── models/
│   │       ├── __init__.py
│   │       ├── agent.py        # agent domain model
│   │       └── task.py         # task domain model
│   │
│   ├── services/               # 服务层（use cases）
│   │   ├── __init__.py
│   │   ├── agent_service.py    # agent业务逻辑
│   │   ├── llm_service.py      # LLM调用包装
│   │   └── task_service.py     # task业务逻辑
│   │
│   ├── infrastructure/         # 基础设施层
│   │   ├── __init__.py
│   │   ├── database/
│   │   │   ├── __init__.py
│   │   │   ├── connection.py
│   │   │   └── repositories.py
│   │   ├── llm/
│   │   │   ├── __init__.py
│   │   │   ├── openai_client.py
│   │   │   ├── claude_client.py
│   │   │   └── cache.py        # LLM response caching
│   │   ├── logging/
│   │   │   ├── __init__.py
│   │   │   ├── setup.py        # 日志配置
│   │   │   └── formatters.py
│   │   └── tracing/
│   │       ├── __init__.py
│   │       └── opentelemetry.py
│   │
│   └── utils/
│       ├── __init__.py
│       ├── async_helpers.py
│       └── retry.py
│
├── tests/
│   ├── __init__.py
│   ├── conftest.py
│   ├── unit/
│   │   ├── test_agent.py
│   │   └── test_services.py
│   ├── integration/
│   │   └── test_api.py
│   └── fixtures/
│
├── docker/
│   ├── Dockerfile
│   └── docker-compose.yml
│
├── .env.example
├── requirements.txt
├── pyproject.toml
└── README.md
```

### 3. 关键代码片段

**配置管理（支持环境隔离）：**

```python
from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    # 应用配置
    APP_NAME: str = "AI Agent Platform"
    DEBUG: bool = False
    LOG_LEVEL: str = "INFO"
    
    # LLM配置
    OPENAI_API_KEY: str
    CLAUDE_API_KEY: str
    LLM_REQUEST_TIMEOUT: int = 30
    LLM_CACHE_ENABLED: bool = True
    
    # 数据库
    DATABASE_URL: str
    DB_POOL_SIZE: int = 20
    DB_MAX_OVERFLOW: int = 10
    
    # 缓存
    REDIS_URL: str = "redis://localhost:6379"
    CACHE_TTL: int = 3600
    
    class Config:
        env_file = ".env"
        env_file_encoding = 'utf-8'
        case_sensitive = True

@lru_cache
def get_settings() -> Settings:
    return Settings()
```

**结构化日志（使用structlog）：**

```python
import structlog
import logging
from pythonjsonlogger import jsonlogger

def setup_logging(config: Settings):
    # 配置structlog
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )
    
    # 配置root logger
    root_logger = logging.getLogger()
    handler = logging.StreamHandler()
    formatter = jsonlogger.JsonFormatter()
    handler.setFormatter(formatter)
    root_logger.addHandler(handler)
    root_logger.setLevel(config.LOG_LEVEL)

logger = structlog.get_logger()

# 使用
logger.info("task_started", task_id=task_id, agent_name=agent_name, extra_data={"retry": 1})
```

**LLM调用的容错机制：**

```python
import asyncio
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)

class LLMService:
    def __init__(self, config: Settings):
        self.config = config
        self.logger = structlog.get_logger()
        self.cache = AsyncCache(redis_url=config.REDIS_URL)
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((asyncio.TimeoutError, ConnectionError)),
    )
    async def call_claude(
        self,
        messages: List[dict],
        model: str = "claude-opus-4-1",
        temperature: float = 0.7,
        max_tokens: int = 2000,
    ) -> str:
        """
        调用Claude API，带缓存和重试
        """
        cache_key = self._get_cache_key(messages, model, temperature, max_tokens)
        
        # 尝试从缓存获取
        if self.config.LLM_CACHE_ENABLED:
            cached_response = await self.cache.get(cache_key)
            if cached_response:
                self.logger.info("llm_cache_hit", cache_key=cache_key)
                return cached_response
        
        try:
            # 实际调用
            response = await asyncio.wait_for(
                self._call_claude_internal(messages, model, temperature, max_tokens),
                timeout=self.config.LLM_REQUEST_TIMEOUT
            )
            
            # 缓存结果
            if self.config.LLM_CACHE_ENABLED:
                await self.cache.set(cache_key, response, ttl=3600)
            
            self.logger.info(
                "llm_call_success",
                model=model,
                input_tokens=len(str(messages)),
            )
            return response
            
        except asyncio.TimeoutError:
            self.logger.error(
                "llm_call_timeout",
                model=model,
                timeout=self.config.LLM_REQUEST_TIMEOUT
            )
            raise
        except Exception as e:
            self.logger.error(
                "llm_call_failed",
                model=model,
                error=str(e),
                error_type=type(e).__name__
            )
            raise
    
    async def _call_claude_internal(self, messages, model, temperature, max_tokens):
        # 实际API调用逻辑
        pass
```

**FastAPI应用的依赖注入和中间件：**

```python
from fastapi import FastAPI, Depends, Request
from fastapi.middleware.cors import CORSMiddleware
import time
import uuid

app = FastAPI(
    title=settings.APP_NAME,
    debug=settings.DEBUG,
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 请求ID追踪中间件
@app.middleware("http")
async def add_request_id_middleware(request: Request, call_next):
    request_id = str(uuid.uuid4())
    request.state.request_id = request_id
    start_time = time.time()
    
    response = await call_next(request)
    
    process_time = time.time() - start_time
    logger.info(
        "http_request",
        request_id=request_id,
        method=request.method,
        path=request.url.path,
        status_code=response.status_code,
        duration_ms=process_time * 1000,
    )
    
    response.headers["X-Request-ID"] = request_id
    return response

# 依赖注入
async def get_agent_service(
    settings: Settings = Depends(get_settings),
) -> AgentService:
    return AgentService(settings)

# 使用
@app.post("/agents/execute")
async def execute_agent(
    request: AgentExecutionRequest,
    agent_service: AgentService = Depends(get_agent_service),
) -> AgentExecutionResponse:
    result = await agent_service.execute(request)
    return result
```

**异步任务处理（Celery + Redis）：**

```python
from celery import Celery
from kombu import Exchange, Queue

celery_app = Celery(
    "ai_agent_platform",
    broker=settings.REDIS_URL,
    backend=settings.REDIS_URL,
)

celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=30 * 60,  # 30分钟
    task_soft_time_limit=25 * 60,
)

@celery_app.task(bind=True, max_retries=3)
def execute_long_running_agent(self, agent_id: str, task_data: dict):
    try:
        logger.info("long_task_started", agent_id=agent_id, task_id=self.request.id)
        result = synchronous_agent_execution(agent_id, task_data)
        logger.info("long_task_completed", task_id=self.request.id)
        return result
    except Exception as exc:
        logger.error("long_task_failed", task_id=self.request.id, error=str(exc))
        raise self.retry(exc=exc, countdown=60)
```

### 4. 测试策略

```python
# tests/unit/test_llm_service.py
import pytest
from unittest.mock import AsyncMock, patch

@pytest.fixture
def llm_service(settings):
    return LLMService(settings)

@pytest.mark.asyncio
async def test_llm_cache_hit(llm_service):
    """测试LLM缓存命中"""
    messages = [{"role": "user", "content": "test"}]
    
    # 第一次调用
    with patch.object(llm_service, '_call_claude_internal', new_callable=AsyncMock) as mock:
        mock.return_value = "response1"
        result1 = await llm_service.call_claude(messages)
    
    # 第二次调用应该从缓存获取
    with patch.object(llm_service, '_call_claude_internal', new_callable=AsyncMock) as mock:
        result2 = await llm_service.call_claude(messages)
        mock.assert_not_called()  # 不应该调用API
    
    assert result1 == result2

@pytest.mark.asyncio
async def test_llm_retry_on_timeout(llm_service):
    """测试超时重试"""
    messages = [{"role": "user", "content": "test"}]
    
    with patch.object(llm_service, '_call_claude_internal', new_callable=AsyncMock) as mock:
        mock.side_effect = [asyncio.TimeoutError(), "success"]
        result = await llm_service.call_claude(messages)
    
    assert result == "success"
    assert mock.call_count == 2
```

## 部署与运维

**Docker化：**

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY app/ ./app

ENV PYTHONUNBUFFERED=1
ENV LOG_LEVEL=INFO

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**docker-compose：**

```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:pass@postgres:5432/aiagent
      - REDIS_URL=redis://redis:6379
      - LOG_LEVEL=INFO
    depends_on:
      - postgres
      - redis
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s

  postgres:
    image: postgres:15
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
      - POSTGRES_DB=aiagent
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data

volumes:
  postgres_data:
  redis_data:
```

